* Entry
** tool
gdb + qemu + riscv + linux-kernel
#+begin_src sh
  add-symbol-file vmlinux -s .head.text 0x80200000
#+end_src
** pic / none-pic
** vmlinux
load section
#+begin_src
    RISCV_ATTRIBUT 0x00000000065b0d6c 0x0000000000000000 0x0000000000000000
                 0x0000000000000037 0x0000000000000000  R      0x1
  LOAD           0x0000000000001000 0xffffffff80000000 0x0000000000000000
                 0x000000000040e6f6 0x000000000040e6f6  R E    0x1000
  LOAD           0x0000000000600000 0xffffffff80600000 0x0000000000600000
                 0x000000000002b560 0x000000000002b560  R E    0x200000
  LOAD           0x000000000062c000 0xffffffff80800000 0x0000000000800000
                 0x0000000000016c18 0x0000000000016c18  RW     0x1000
  LOAD           0x0000000000643000 0xffffffff80a00000 0x0000000000a00000
                 0x0000000000161844 0x0000000000161844  RW     0x1000
  LOAD           0x00000000007a5000 0xffffffff80c00000 0x0000000000c00000
                 0x0000000000000698 0x0000000000000698  R      0x1000
  LOAD           0x00000000007a6000 0xffffffff80e00000 0x0000000000e00000
                 0x00000000000be800 0x00000000001010ad  RW     0x1000
  NOTE           0x00000000007a47f0 0xffffffff80b617f0 0x0000000000b617f0
                 0x0000000000000054 0x0000000000000054  R      0x4
  GNU_STACK      0x0000000000000000 0x0000000000000000 0x0000000000000000
                 0x0000000000000000 0x0000000000000000  RW     0x10


#+end_src

Sections
#+begin_src
    [Nr] Name              Type             Address           Offset
       Size              EntSize          Flags  Link  Info  Align
  [ 0]                   NULL             0000000000000000  00000000
       0000000000000000  0000000000000000           0     0     0
  [ 1] .head.text        PROGBITS         ffffffff80000000  00001000
       0000000000001e90  0000000000000000  AX       0     0     4096
  [ 2] .text             PROGBITS         ffffffff80002000  00003000
       000000000040c6f6  0000000000000000  AX       0     0     4
  [ 3] .init.text        PROGBITS         ffffffff80600000  00600000
       000000000002a3d6  0000000000000000  AX       0     0     2097152
  [ 4] .exit.text        PROGBITS         ffffffff8062a3d8  0062a3d8
       0000000000001188  0000000000000000  AX       0     0     2
  [ 5] .init.data        PROGBITS         ffffffff80800000  0062c000
       000000000000ec58  0000000000000000  WA       0     0     4096
  [ 6] .data..percpu     PROGBITS         ffffffff8080f000  0063b000
       00000000000079d8  0000000000000000  WA       0     0     64
  [ 7] .alternative      PROGBITS         ffffffff808169d8  006429d8
       0000000000000240  0000000000000000   A       0     0     1
  [ 8] .rodata           PROGBITS         ffffffff80a00000  00643000
       000000000011eb70  0000000000000000  WA       0     0     64
  [ 9] .pci_fixup        PROGBITS         ffffffff80b1eb70  00761b70
       0000000000003b58  0000000000000000   A       0     0     8

#+end_src
** head.S
*** _start
_start symbol defined in arch/riscv/kernel/head.S
if we defined the CONFIG_EFI, then load s4 = -13 this required by UEFI.
#+begin_src asm
  __HEAD
  ENTRY(_start)
  	/*
  	 ,* Image header expected by Linux boot-loaders. The image header data
  	 ,* structure is described in asm/image.h.
  	 ,* Do not modify it without modifying the structure and all bootloaders
  	 ,* that expects this header format!!
  	 ,*/
  #ifdef CONFIG_EFI  // actually we define this Macro
  	/*
  	 ,* This instruction decodes to "MZ" ASCII required by UEFI.
  	 ,*/
  	c.li s4,-13
  	j _start_kernel
  #else
  	/* jump to start kernel */
  	j _start_kernel
  	/* reserved */
  	.word 0
  #endif
#+end_src

*** _start_kernel
arch/riscv/kernel/head.S
**** disable interrupts
#+begin_src asm
  ENTRY(_start_kernel)
  	/* Mask all interrupts */
  	csrw CSR_IE, zero
  	csrw CSR_IP, zero
#+end_src
**** load global pointer
#+begin_src asm
  	/* Load the global pointer */
  .option push
  .option norelax
  	la gp, __global_pointer$
  .option pop
#+end_src
The __global_pointer$ defines in arch/riscv/kernel/linux/vmlinux.lds file, it pointes to the .sdata VMA address plus 0x800. Actually, using la to load address, when the pic is enabled, it is relative pc address.
#+begin_src sh
 .sdata : {
  __global_pointer$ = . + 0x800;
  *(.sdata*)
 }
#+end_src
**** disable fpu
#+begin_src asm
  	/*
  	 ,* Disable FPU to detect illegal usage of
  	 ,* floating point in kernel space
  	 ,*/
  	li t0, SR_FS
  	csrc CSR_STATUS, t0
#+end_src

**** SMP configuration
The CONFIG_NR_CPUS defined in arch/riscv/Kconfig, it is a configuration of SMP,
*platform type->*
  *Maximum number of CPUS*
#+begin_src asm
#ifdef CONFIG_SMP
	li t0, CONFIG_NR_CPUS
	blt a0, t0, .Lgood_cores
	tail .Lsecondary_park
.Lgood_cores:
#endif
#+end_src
The a0 stores the mhartid of cpuid that runs the kernel, if a0 < t0 here 8, the jump .Lgood_cores, we see it as BSP, then use lottery method to let one core to run the coldboot of kernel.
**** lottery cold_boot
Using *amoadd* instruction to make the lottery happen.
#+begin_src asm
#ifndef CONFIG_XIP_KERNEL
    /* Pick one hart to run the main boot sequence */
    la a3, hart_lottery
    li a2, 1
    amoadd.w a3, a2, (a3)
    bnez a3, .Lsecondary_start
#else
#+end_src
If one cpu doesn't get lottery it will jumps to .Lsecondary_start, if gets the lottery, it will then clear the bss doing the main coldboot of kernel.
**** clear bss section
#+begin_src asm
#ifndef CONFIG_XIP_KERNEL
    /* Clear BSS for flat non-ELF images */
    la a3, __bss_start
    la a4, __bss_stop
    ble a4, a3, clear_bss_done
clear_bss:
    REG_S zero, (a3)
    add a3, a3, RISCV_SZPTR
    blt a3, a4, clear_bss
clear_bss_done:
#endif
#+end_src
**** save harid and dtb phy address
a0 is the hartid, and a1 stores the dtb physical address.
#+begin_src asm
    /* Save hart ID and DTB physical address */
    mv s0, a0
    mv s1, a1
#+end_src
**** set the bootid number to boot_cpu_hartid 
#+begin_src asm
    la a2, boot_cpu_hartid
    XIP_FIXUP_OFFSET a2
    REG_S a0, (a2)
#+end_src
**** init_task
defined in init/init_task.c file, using riscv tp register to points to it, and then do some initialization.
struct task_struct is defined in *include/linux/sched.h*.
#+begin_src c
struct task_struct init_task
#ifdef CONFIG_ARCH_TASK_STRUCT_ON_STACK
    __init_task_data
#endif
    __aligned(L1_CACHE_BYTES)
= {} EXPORT_SYMBOL(init_task)
#+end_src
The init_thread_union defined in arch/riscv/kernel/vmlinux.lds file, these instructions are aimed to setup the first task, sp points to stack of init_task, and tp points to init_task, and now a0 is the dtb phy address.
#+begin_src asm
    /* Initialize page tables and relocate to virtual addresses */
    la tp, init_task
    la sp, init_thread_union + THREAD_SIZE
    XIP_FIXUP_OFFSET sp
    mv a0, s1
#+end_src
The thread_union is defined in *include/linux/sched.h*, by searching the menuconfig, we can see that the CONFIG_ARCH_TASK_STRUCT_ON_STACK is false, and the CONFIG_THREAD_INFO_IN_TASK is true.
#+begin_src c
union thread_union {
#ifndef CONFIG_ARCH_TASK_STRUCT_ON_STACK
	struct task_struct task;
#endif
#ifndef CONFIG_THREAD_INFO_IN_TASK
	struct thread_info thread_info;
#endif
	unsigned long stack[THREAD_SIZE/sizeof(long)];
};
#+end_src
The task_stack is defined and allocated space in linker file, the kernel uses a union struct to store both the thread info and the thread stack. But we don't see the definition of the stack in any .c files.

init_stack ->
init_task   ->
.data..init_task not defined
.data..init_thread_info
init_thread_union
#+begin_src sh
__start_init_task = .; init_thread_union = .; init_stack = .; KEEP(*(.data..init_task)) KEEP(*(.data..init_thread_info)) . = __start_init_task + ((1 << (12)) << (2 + 0)); __end_init_task = .;
#+end_src

#+begin_src c
arch/riscv/include/asm/thread_info.h
#ifdef CONFIG_KASAN (0)
#define KASAN_STACK_ORDER 1
#else
#define KASAN_STACK_ORDER 0
#endif

/* thread information allocation */
#ifdef CONFIG_64BIT
#define THREAD_SIZE_ORDER	(2 + KASAN_STACK_ORDER)
#else
#define THREAD_SIZE_ORDER	(1 + KASAN_STACK_ORDER)
#endif
#define THREAD_SIZE		(PAGE_SIZE << THREAD_SIZE_ORDER)

arch/riscv/include/asm/page.h
#define PAGE_SHIFT	(12)
#define PAGE_SIZE	(_AC(1, UL) << PAGE_SHIFT)
#define PAGE_MASK	(~(PAGE_SIZE - 1))
#+end_src

**** setup vm before relocate
Firstly set the stvec to hold the vector entry in order to help debug, the setup_vm function is arch related which is defined in *arch/riscv/mm/init.c*
#+begin_src asm
    /* Set trap vector to spin forever to help debug */
    la a3, .Lsecondary_park
    csrw CSR_TVEC, a3
    call setup_vm
#+end_src
***** initialize the kernel_mapping

#+begin_src c
//arch/riscv/mm/init.c
struct kernel_mapping kernel_map __ro_after_init;
EXPORT_SYMBOL(kernel_map);
//arch/riscv/include/asm/page.h:
struct kernel_mapping {
    unsigned long virt_addr;
    uintptr_t phys_addr;
    uintptr_t size;
    /* Offset between linear mapping virtual address and kernel load address */
    unsigned long va_pa_offset;
    /* Offset between kernel mapping virtual address and kernel load address */
    unsigned long va_kernel_pa_offset;
    unsigned long va_kernel_xip_pa_offset;
#ifdef CONFIG_XIP_KERNEL
    uintptr_t xiprom;
    uintptr_t xiprom_sz;
#endif
};
#+end_src
***** initialize the pt_alloc_ops
#+begin_src c
#ifdef CONFIG_MMU
static struct pt_alloc_ops _pt_ops __initdata;

#ifdef CONFIG_XIP_KERNEL
#define pt_ops (*(struct pt_alloc_ops *)XIP_FIXUP(&_pt_ops))
#else
#define pt_ops _pt_ops
#+end_src
***** initialize the pgdir table
#+begin_src c
./arch/riscv/include/asm/pgtable.h:#define PTRS_PER_PGD    (PAGE_SIZE / sizeof(pgd_t))
pgd_t early_pg_dir[PTRS_PER_PGD] __initdata __aligned(PAGE_SIZE);
#+end_src
create_pgd_mapping
#+begin_src c

    /* Setup early PGD for fixmap */
    create_pgd_mapping(early_pg_dir, FIXADDR_START,
                        (uintptr_t)fixmap_pgd_next, PGDIR_SIZE, PAGE_TABLE);

#ifndef __PAGETABLE_PMD_FOLDED // this macro is defined for 32-bit arch
    /* Setup fixmap PMD */
    create_pmd_mapping(fixmap_pmd, FIXADDR_START,
                        (uintptr_t)fixmap_pte, PMD_SIZE, PAGE_TABLE);
    /* Setup trampoline PGD and PMD */
    create_pgd_mapping(trampoline_pg_dir, kernel_map.virt_addr,
                        (uintptr_t)trampoline_pmd, PGDIR_SIZE, PAGE_TABLE);

    create_pmd_mapping(trampoline_pmd, kernel_map.virt_addr,
			   kernel_map.phys_addr, PMD_SIZE, PAGE_KERNEL_EXEC);
#+end_src
**** relocate early_pg_dir
#+begin_src asm
#ifdef CONFIG_MMU
    la a0, early_pg_dir
    XIP_FIXUP_OFFSET a0
    call relocate
#endif /* CONFIG_MMU */
#+end_src

add ra with pa_va_offset when this function returns, it will goes around high address.
#+begin_src asm
.align 2
#ifdef CONFIG_MMU
relocate:
	/* Relocate return address */
	la a1, kernel_map
	XIP_FIXUP_OFFSET a1
	REG_L a1, KERNEL_MAP_VIRT_ADDR(a1)
	la a2, _start
	sub a1, a1, a2
	add ra, ra, a1
#+end_src
In the same time, relocate the stvec to vma using add offset to it.
#+begin_src asm
	/* Point stvec to virtual address of intruction after satp write */
	la a2, 1f
	add a2, a2, a1
	csrw CSR_TVEC, a2
.align 2
1:
	/* Set trap vector to spin forever to help debug */
	la a0, .Lsecondary_park
	csrw CSR_TVEC, a0
#+end_src
Compute the satp value, actually the linux kernel use sv39 page-mapping technique.
#+begin_src asm
	/* Compute satp for kernel page tables, but don't load it yet */
	srl a2, a0, PAGE_SHIFT
	li a1, SATP_MODE
	or a2, a2, a1
#+end_src
Load trampoline entry_pgdir to satp, instead of early_pg_dir, which will cause a instruction page_fault.
#+begin_src asm
	/*
	 * Load trampoline page directory, which will cause us to trap to
	 * stvec if VA != PA, or simply fall through if VA == PA.  We need a
	 * full fence here because setup_vm() just wrote these PTEs and we need
	 * to ensure the new translations are in use.
	 */
	la a0, trampoline_pg_dir
	XIP_FIXUP_OFFSET a0
	srl a0, a0, PAGE_SHIFT
	or a0, a0, a1
	sfence.vma
	csrw CSR_SATP, a0
#+end_src
After setting up trampoline_pg_dir, we can use qemu info mem command to check, the page mappings like below.
#+begin_src sh
ffffffff80000000 0000000080200000 0000000000200000 rwx-gad
#+end_src
So, when exectues the next instruction below csrw, it will cause page-fault, then in the page fault-handler, the kernel will reassign the satp using the early_pg_dir here below.
#+begin_src asm
1:
	/* Set trap vector to spin forever to help debug */
	la a0, .Lsecondary_park
	csrw CSR_TVEC, a0

	/* Reload the global pointer */
.option push
.option norelax
	la gp, __global_pointer$
.option pop

	/*
	 * Switch to kernel page tables.  A full fence is necessary in order to
	 * avoid using the trampoline translations, which are only correct for
	 * the first superpage.  Fetching the fence is guarnteed to work
	 * because that first superpage is translated the same way.
	 */
	csrw CSR_SATP, a2
	sfence.vma

	ret
  	
#+end_src

#+begin_src sh
ffffffcefec00000 00000000bf600000 0000000000400000 rw--gad
ffffffff80000000 0000000080200000 0000000001000000 rwx-gad
#+end_src
The linux kernel using this method to avoid the identical mapping which can reduce memory usage.

**** setup_trap_vector

ENTRY(handle_exception) defined in *arch/riscv/kernel/entry.S*
#+begin_src asm
.align 2
setup_trap_vector:
	/* Set trap vector to exception handler */
	la a0, handle_exception
	csrw CSR_TVEC, a0

	/*
	 * Set sup0 scratch register to 0, indicating to exception vector that
	 * we are presently executing in kernel.
	 */
	csrw CSR_SCRATCH, zero
	ret
#+end_src
**** restore C environment
restore tp and sp
#+begin_src asm
    /* Restore C environment */
    la tp, init_task
    sw zero, TASK_TI_CPU(tp) //thread_info member
    la sp, init_thread_union + THREAD_SIZE
#+end_src
**** soc_early_init
This function is defined in *arch/riscv/kernel/soc.c*
#+begin_src c
      /* Start the kernel */
      call soc_early_init
  /*
   ,* This is called extremly early, before parse_dtb(), to allow initializing
   ,* SoC hardware before memory or any device driver initialization.
   ,*/
  void __init soc_early_init(void)
  {
  	void (*early_fn)(const void *fdt);
  	const struct of_device_id *s;
  	const void *fdt = dtb_early_va;

  	for (s = (void *)&__soc_early_init_table_start;
  	     (void *)s < (void *)&__soc_early_init_table_end; s++) {
  		if (!fdt_node_check_compatible(fdt, 0, s->compatible)) {
  			early_fn = s->data;
  			early_fn(fdt);
  			return;
  		}
  	}
  }
#+end_src
**** start kernel
using tail which is presudo instruction of jal zero, start_kernel, discarding the return address. defined in *init/main.c*
#+begin_src asm
  tail start_kernel
asmlinkage __visible void __init __no_sanitize_address start_kernel(void)
{}
#+end_src



* start_kernel
** init sequence
** arch_call_rest_init
#+begin_src c
void __init __weak arch_call_rest_init(void)
{
	rest_init();
}
#+end_src
** rest_init
#+begin_src c
/*
 * We need to finalize in a non-__init function or else race conditions
 * between the root thread and the init thread may cause start_kernel to
 * be reaped by free_initmem before the root thread has proceeded to
 * cpu_idle.
 *
 * gcc-3.4 accidentally inlines this function, so use noinline.
 */
noinline void __ref rest_init(void)
{
	struct task_struct *tsk;
	int pid;

	rcu_scheduler_starting();
	/*
	 * We need to spawn init first so that it obtains pid 1, however
	 * the init task will end up wanting to create kthreads, which, if
	 * we schedule it before we create kthreadd, will OOPS.
	 */
	pid = kernel_thread(kernel_init, NULL, CLONE_FS);
	/*
	 * Pin init on the boot CPU. Task migration is not properly working
	 * until sched_init_smp() has been run. It will set the allowed
	 * CPUs for init to the non isolated CPUs.
	 */
	rcu_read_lock();
	tsk = find_task_by_pid_ns(pid, &init_pid_ns);
	tsk->flags |= PF_NO_SETAFFINITY;
	set_cpus_allowed_ptr(tsk, cpumask_of(smp_processor_id()));
	rcu_read_unlock();

	numa_default_policy();
	pid = kernel_thread(kthreadd, NULL, CLONE_FS | CLONE_FILES);
	rcu_read_lock();
	kthreadd_task = find_task_by_pid_ns(pid, &init_pid_ns);
	rcu_read_unlock();

	/*
	 * Enable might_sleep() and smp_processor_id() checks.
	 * They cannot be enabled earlier because with CONFIG_PREEMPTION=y
	 * kernel_thread() would trigger might_sleep() splats. With
	 * CONFIG_PREEMPT_VOLUNTARY=y the init task might have scheduled
	 * already, but it's stuck on the kthreadd_done completion.
	 */
	system_state = SYSTEM_SCHEDULING;

	complete(&kthreadd_done);

	/*
	 * The boot idle thread must execute schedule()
	 * at least once to get things moving:
	 */
	schedule_preempt_disabled();
	/* Call into cpu_idle with preempt disabled */
	cpu_startup_entry(CPUHP_ONLINE);
}
#+end_src
** kernel_init
The pid1 process is *init*. The init process first runs in kernel space, then it read the init from disk space and runs in user space, after that the system is booted up.
** kthreadd
PID 2 process kthreadd, the job of this process is mainly for schedule process and etc.


* init

* kthreadd
